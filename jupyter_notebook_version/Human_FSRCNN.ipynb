{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tarfile\n",
    "\n",
    "from os import listdir,remove\n",
    "from os.path import exists, join, basename\n",
    "\n",
    "from six.moves import urllib\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "from math import log10\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import easydict      #rd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prepare Data                                 <br>\n",
    "Download data if can't find it in local directory           <br>\n",
    "Get training set from  './dataset/HR_img_train', get testset from  './dataset/HR_img_test'     <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bsd300(dest=\".//dataset\"):\n",
    "    output_image_dir = join(dest, \"BSDS300/images\")\n",
    "\n",
    "    if not exists(output_image_dir):\n",
    "        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n",
    "        print(\"downloading url \", url)\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "\n",
    "        file_path = join(dest, basename(url))\n",
    "        with open(file_path, 'wb') as file:\n",
    "            print(\"Extracting data\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            for item in tar:\n",
    "                tar.extract(item, dest)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "    return output_image_dir    #rd\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)    #rd    what does this mean?\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])  #rd\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('YCbCr')\n",
    "    y, _, _ = img.split()\n",
    "    return y\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, input_transform=None, target_transform=None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_image = load_img(self.image_filenames[index])\n",
    "        target = input_image.copy()\n",
    "        # print(target.size)\n",
    "        if self.input_transform:\n",
    "            input_image = self.input_transform(input_image)\n",
    "        else:\n",
    "            transi = Compose([\n",
    "                            Resize((input_image.size[1] // 4, input_image.size[0] // 4), Image.BICUBIC),\n",
    "                            ToTensor()\n",
    "                            ])\n",
    "            input_image = transi(input_image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        else:\n",
    "            transt = Compose([\n",
    "                ToTensor()\n",
    "            ])\n",
    "            target = transt(target)\n",
    "\n",
    "        # print(input_image.size(), target.size())\n",
    "        return input_image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):   #get the low resolution input by original image.  This is a function, return can be viewed\n",
    "                                                  # as a set of parameters import into DatasetFromFolder.input_trainsform and target_transform\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),                        #get a small size of data, Corp with less pieces and resize corp? what does corp mean \n",
    "                                                      # in this project? why not whole img\n",
    "        Resize(crop_size // upscale_factor),           # Resize is what? any help? a way to down size? is it reliable?\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(upscale_factor):\n",
    "    # root_dir = download_bsd300()\n",
    "    root_dir = './/dataset//'\n",
    "    train_dir = join(root_dir, \"HR_img_train//\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=None,\n",
    "                             target_transform=None\n",
    "                             # input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             # target_transform=target_transform(crop_size))\n",
    "                             )\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor):\n",
    "    # root_dir = download_bsd300()\n",
    "    root_dir = './/dataset//'\n",
    "    test_dir = join(root_dir, \"HR_img_test//\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=None,\n",
    "                             target_transform=None\n",
    "                             # input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             # target_transform=target_transform(crop_size))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training precedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BAR_LENGTH = 80\n",
    "LAST_T = time.time()\n",
    "BEGIN_T = LAST_T\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global LAST_T, BEGIN_T\n",
    "    if current == 0:\n",
    "        BEGIN_T = time.time()  # Reset for new bar.\n",
    "\n",
    "    current_len = int(TOTAL_BAR_LENGTH * (current + 1) / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - current_len) - 1\n",
    "\n",
    "    sys.stdout.write(' %d/%d' % (current + 1, total))\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(current_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    current_time = time.time()\n",
    "    step_time = current_time - LAST_T\n",
    "    LAST_T = current_time\n",
    "    total_time = current_time - BEGIN_T\n",
    "\n",
    "    time_used = '  Step: %s' % format_time(step_time)\n",
    "    time_used += ' | Tot: %s' % format_time(total_time)\n",
    "    if msg:\n",
    "        time_used += ' | ' + msg\n",
    "\n",
    "    msg = time_used\n",
    "    sys.stdout.write(msg)\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "# return the formatted time\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    seconds_final = int(seconds)\n",
    "    seconds = seconds - seconds_final\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    output = ''\n",
    "    time_index = 1\n",
    "    if days > 0:\n",
    "        output += str(days) + 'D'\n",
    "        time_index += 1\n",
    "    if hours > 0 and time_index <= 2:\n",
    "        output += str(hours) + 'h'\n",
    "        time_index += 1\n",
    "    if minutes > 0 and time_index <= 2:\n",
    "        output += str(minutes) + 'm'\n",
    "        time_index += 1\n",
    "    if seconds_final > 0 and time_index <= 2:\n",
    "        output += str(seconds_final) + 's'\n",
    "        time_index += 1\n",
    "    if millis > 0 and time_index <= 2:\n",
    "        output += str(millis) + 'ms'\n",
    "        time_index += 1\n",
    "    if output == '':\n",
    "        output = '0ms'\n",
    "    return output                      #rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Built CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_channels, upscale_factor, d=64, s=12, m=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.first_part = nn.Sequential(nn.Conv2d(in_channels=num_channels, out_channels=d, kernel_size=5, stride=1, padding=2),\n",
    "                                        nn.PReLU())\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=d, out_channels=s, kernel_size=1, stride=1, padding=0), nn.PReLU()))\n",
    "        for _ in range(m):\n",
    "\n",
    "            # self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1))      # Normal conv\n",
    "\n",
    "            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1, groups=s))      # MobileNet conv\n",
    "            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        self.layers.append(nn.PReLU())\n",
    "        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=s, out_channels=d, kernel_size=1, stride=1, padding=0), nn.PReLU()))\n",
    "\n",
    "        self.mid_part = torch.nn.Sequential(*self.layers)\n",
    "\n",
    "        self.last_part = nn.ConvTranspose2d(in_channels=d, out_channels=num_channels, kernel_size=9, stride=upscale_factor, padding=3, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.first_part(x)\n",
    "        out = self.mid_part(out)\n",
    "        out = self.last_part(out)\n",
    "        return out\n",
    "\n",
    "    def weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(mean, std)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0.0, 0.0001)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "class FSRCNNTrainer(object):\n",
    "    def __init__(self, config, training_loader, testing_loader):\n",
    "        super(FSRCNNTrainer, self).__init__()\n",
    "        self.CUDA = torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda' if self.CUDA else 'cpu')\n",
    "        self.model = None\n",
    "        self.lr = config.lr\n",
    "        self.nEpochs = config.nEpochs\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.seed = config.seed\n",
    "        self.upscale_factor = config.upscale_factor\n",
    "        self.training_loader = training_loader\n",
    "        self.testing_loader = testing_loader\n",
    "        self.testpsnr = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Net(num_channels=1, upscale_factor=self.upscale_factor).to(self.device)\n",
    "        self.model.weight_init(mean=0.0, std=0.2)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        if self.CUDA:\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "            cudnn.benchmark = True\n",
    "            self.criterion.cuda()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[50, 75, 100], gamma=0.5)  # lr decay\n",
    "\n",
    "    def save_model(self):\n",
    "        model_out_path = \"model_path.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_num, (data, target) in enumerate(self.training_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(self.model(data), target)\n",
    "            train_loss += loss.item()\n",
    "            # print(train_loss / (batch_num + 1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #print('batch_num:', batch_num, '/', len(self.training_loader), 'Loss:', train_loss / (batch_num + 1))\n",
    "            progress_bar(batch_num, len(self.training_loader), 'Loss: %.4f' % (train_loss / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average Loss: {:.4f}\".format(train_loss / len(self.training_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        avg_psnr = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (data, target) in enumerate(self.testing_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                prediction = self.model(data)\n",
    "                mse = self.criterion(prediction, target)\n",
    "                psnr = 10 * log10(1 / mse.item())\n",
    "                avg_psnr += psnr\n",
    "        print('test PSNR:', avg_psnr / len(self.testing_loader))\n",
    "                # progress_bar(batch_num, len(self.testing_loader), 'PSNR: %.4f' % (avg_psnr / (batch_num + 1)))\n",
    "\n",
    "        save_path = './saved_models'\n",
    "        if avg_psnr / len(self.testing_loader) > self.testpsnr:\n",
    "            self.testpsnr = avg_psnr / len(self.testing_loader)\n",
    "            folder = os.path.exists(save_path)\n",
    "            if not folder:\n",
    "                os.makedirs(save_path)\n",
    "                print('create folder to save models')\n",
    "            torch.save(self.model, save_path + '/model_' + str(self.testpsnr))\n",
    "            print('model saved ......')\n",
    "        print(\"    Average PSNR: {:.4f} dB\".format(avg_psnr / len(self.testing_loader)))\n",
    "\n",
    "    def run(self):\n",
    "        self.build_model()\n",
    "        print('Total parameters:', sum(p.numel() for p in self.model.parameters()))\n",
    "        print('Total trainable parameters:', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
    "        for epoch in range(1, self.nEpochs + 1):\n",
    "            print(\"\\n===> Epoch {} starts:\".format(epoch))\n",
    "            self.train()\n",
    "            self.test()\n",
    "            self.scheduler.step(epoch)\n",
    "            \n",
    " #           check=0\n",
    "#             if avg_psnr>31:\n",
    "#                 check=1\n",
    "#             if avg_psnr<=31& check==1:\n",
    "#                 self.save_model()\n",
    "#                 print('model saved with psnr=%f'%avg_psnr)\n",
    "#                 break\n",
    "#             if avg_psnr>35:\n",
    "#                 self.save_model()\n",
    "#                 print('model saved with psnr=%f'%avg_psnr)\n",
    "#                 break\n",
    "#             elif epoch == self.nEpochs:\n",
    "#                 self.save_model()\n",
    "#                 print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model in 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading datasets\n",
      "Total parameters: 9569\n",
      "Total trainable parameters: 9569\n",
      "\n",
      "===> Epoch 1 starts:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-59e0a158714e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-184-59e0a158714e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFSRCNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-3b7494883c76>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnEpochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n===> Epoch {} starts:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-3b7494883c76>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m# print(train_loss / (batch_num + 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-3b7494883c76>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmid_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "#parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "# hyper-parameters\n",
    "#parser.add_argument('--batchSize', type=int, default=4, help='training batch size')\n",
    "#parser.add_argument('--testBatchSize', type=int, default=1, help='testing batch size')\n",
    "#parser.add_argument('--nEpochs', type=int, default=20, help='number of epochs to train for')\n",
    "#parser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')\n",
    "#parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "# model configuration\n",
    "#parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'batchSize':1,\n",
    "    'testBatchSize':1,\n",
    "   'nEpochs':15,\n",
    "    'lr':0.001,\n",
    "    'seed':123,\n",
    "    'upscale_factor':4,\n",
    "    'uf':4\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ===========================================================\n",
    "    # Set train dataset & test dataset\n",
    "    # ===========================================================\n",
    "    print('===> Loading datasets')\n",
    "    train_set = get_training_set(args.upscale_factor)\n",
    "    test_set = get_test_set(args.upscale_factor)\n",
    "    training_data_loader = DataLoader(dataset=train_set, batch_size=args.batchSize, shuffle=True)\n",
    "    testing_data_loader = DataLoader(dataset=test_set, batch_size=args.testBatchSize, shuffle=False)\n",
    "\n",
    "    model = FSRCNNTrainer(args, training_data_loader, testing_data_loader)\n",
    "\n",
    "    model.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output high resolution image and compute PSNR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output high resolution image to output_dir, with a specified model\n",
    "def output_image(input_dir,input_model,output_dir):\n",
    "#     test = easydict.EasyDict({\n",
    "#         'input':'.//test_image//1.jpg',\n",
    "#         'model':'model_path.pth',              # the loaded model, model should be saved at project folder './mdoel.pth'\n",
    "#        'output':'.//test_image//1_generate_2.jpg',    \n",
    "#     })\n",
    "\n",
    "# ===========================================================\n",
    "# input image setting\n",
    "# ===========================================================\n",
    "    start = time.time()\n",
    "\n",
    "    GPU_IN_USE = torch.cuda.is_available()\n",
    "    img = Image.open(input_dir).convert('YCbCr')\n",
    "    y, cb, cr = img.split()\n",
    "\n",
    "# ===========================================================\n",
    "# model import & setting\n",
    "# ===========================================================\n",
    "    device = torch.device('cuda' if GPU_IN_USE else 'cpu')\n",
    "    model = torch.load(input_model, map_location=lambda storage, loc: storage)\n",
    "    model = model.to(device)\n",
    "    data = (ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n",
    "    data = data.to(device)\n",
    "\n",
    "    if GPU_IN_USE:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# output and save image\n",
    "# ===========================================================\n",
    "    out = model(data)\n",
    "    out = out.cpu()\n",
    "    out_img_y = out.data[0].numpy()\n",
    "    out_img_y *= 255.0\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
    "\n",
    "    out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
    "\n",
    "    out_img.save(output_dir)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    print('output image saved to:', output_dir,'   running time:',elapsed,'s')\n",
    "\n",
    "# output high resolution image to output_dir, with the current model\n",
    "def creat_image(input_dir,output_dir):\n",
    "    start = time.time()\n",
    "    GPU_IN_USE = torch.cuda.is_available()\n",
    "    img = Image.open(input_dir).convert('YCbCr')\n",
    "    y, cb, cr = img.split()\n",
    "    \n",
    "\n",
    " #   model = torch.load(input_model, map_location=lambda storage, loc: storage)\n",
    " #   model = model.to(device)\n",
    "    data = (ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n",
    "    data = data.to(device)\n",
    "\n",
    "    if GPU_IN_USE:\n",
    "        cudnn.benchmark = True\n",
    "    out = model(data)\n",
    "    out = out.cpu()\n",
    "    out_img_y = out.data[0].numpy()\n",
    "    out_img_y *= 255.0\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
    "\n",
    "    out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
    "\n",
    "    out_img.save(output_dir)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    print('output image saved to:', output_dir,'   running time:',elapsed,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output image saved to: .//test_image//0_generate_3.jpg running time: 2.605011463165283 s\n"
     ]
    }
   ],
   "source": [
    "output_image(input_dir='.//test_image//0.jpg',input_model='model_path2.pth',output_dir='.//test_image//0_generate_3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test of PSNR comparation\n",
    "def psnr(img1, img2):\n",
    "    mse = np.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = max(img1.shape[0],img1.shape[1])\n",
    "    return 10 * log10(PIXEL_MAX / mse)\n",
    "#    return 10 * log10(1 / mse)\n",
    "    \n",
    "def PSNR(target_dir,original_dir):\n",
    "\n",
    "    original = cv2.imread(original_dir)\n",
    "    contrast = cv2.imread(target_dir)\n",
    "    \n",
    "    d=psnr(original,contrast)\n",
    "    print('PSNR=',d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output image saved to: .//LR_img_output//new_0.jpg    running time: 1.3434069156646729 s\n",
      "output image saved to: .//LR_img_output//new_1.jpg    running time: 1.0421850681304932 s\n",
      "output image saved to: .//LR_img_output//new_10.jpg    running time: 0.8896217346191406 s\n"
     ]
    }
   ],
   "source": [
    "image_dir='.//LR_img_0.25'\n",
    "image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "original_dir='.//HR_img_4//'\n",
    "\n",
    "GPU_IN_USE = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU_IN_USE else 'cpu')\n",
    "model = torch.load('.//saved_models//model_33.pth', map_location=lambda storage, loc: storage)\n",
    "model = model.to(device)\n",
    "   \n",
    "for i,filename in enumerate (image_filenames):\n",
    "    img_name=filename.split(os.sep)[-1]\n",
    "    creat_image(input_dir=filename,output_dir='.//LR_img_output//'+img_name)\n",
    "  #  print('high resolution by CNN')\n",
    "   # PSNR('.//HR_img_4//'+filename+'.jpg','.//LR_img_output//'+filename+'.jpg')\n",
    "   # print('high resolution by BICUIBIC')\n",
    "   # PSNR('.//HR_img_4//'+filename+'.jpg','.//LR_img_BICUIBIC//'+filename+'.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//LR_img_output//0.jpg high resolution by CNN\n",
      "16.27083686424689\n",
      ".//LR_img_output//1.jpg high resolution by CNN\n",
      "15.861659877553185\n",
      ".//LR_img_output//10.jpg high resolution by CNN\n",
      "18.275048959429615\n",
      ".//LR_img_output//100.jpg high resolution by CNN\n",
      "16.040215030275483\n",
      ".//LR_img_output//101.jpg high resolution by CNN\n",
      "19.14871264652683\n",
      ".//LR_img_output//102.jpg high resolution by CNN\n",
      "17.68598291436126\n",
      ".//LR_img_output//103.jpg high resolution by CNN\n",
      "18.355294170829158\n",
      ".//LR_img_output//104.jpg high resolution by CNN\n",
      "19.102057471809392\n",
      ".//LR_img_output//105.jpg high resolution by CNN\n",
      "11.948134469080374\n",
      ".//LR_img_output//106.jpg high resolution by CNN\n",
      "17.453424681585588\n",
      ".//LR_img_output//107.jpg high resolution by CNN\n",
      "17.81386131065656\n",
      ".//LR_img_output//108.jpg high resolution by CNN\n",
      "18.52581514150305\n",
      ".//LR_img_output//109.jpg high resolution by CNN\n",
      "13.165928528115018\n",
      ".//LR_img_output//11.jpg high resolution by CNN\n",
      "19.383291008033115\n",
      ".//LR_img_output//110.jpg high resolution by CNN\n",
      "19.35636073497009\n",
      ".//LR_img_output//111.jpg high resolution by CNN\n",
      "18.22341090248054\n",
      ".//LR_img_output//112.jpg high resolution by CNN\n",
      "17.012997224514116\n",
      ".//LR_img_output//113.jpg high resolution by CNN\n",
      "17.683215750720947\n",
      ".//LR_img_output//114.jpg high resolution by CNN\n",
      "20.98048083838585\n",
      ".//LR_img_output//115.jpg high resolution by CNN\n",
      "12.409886953890215\n",
      ".//LR_img_output//116.jpg high resolution by CNN\n",
      "16.97531456426456\n",
      ".//LR_img_output//117.jpg high resolution by CNN\n",
      "15.61192309053553\n",
      ".//LR_img_output//118.jpg high resolution by CNN\n",
      "15.593098204303457\n",
      ".//LR_img_output//119.jpg high resolution by CNN\n",
      "17.201756112018003\n",
      ".//LR_img_output//12.jpg high resolution by CNN\n",
      "12.255609249671869\n",
      ".//LR_img_output//120.jpg high resolution by CNN\n",
      "14.89019242665454\n",
      ".//LR_img_output//121.jpg high resolution by CNN\n",
      "16.108570774106937\n",
      ".//LR_img_output//122.jpg high resolution by CNN\n",
      "12.005458752324165\n",
      ".//LR_img_output//123.jpg high resolution by CNN\n",
      "13.77058612695253\n",
      ".//LR_img_output//124.jpg high resolution by CNN\n",
      "15.57636267953817\n",
      ".//LR_img_output//125.jpg high resolution by CNN\n",
      "20.52250509605484\n",
      ".//LR_img_output//126.jpg high resolution by CNN\n",
      "15.20427178585441\n",
      ".//LR_img_output//127.jpg high resolution by CNN\n",
      "14.018167439974105\n",
      ".//LR_img_output//128.jpg high resolution by CNN\n",
      "14.242707670040758\n",
      ".//LR_img_output//129.jpg high resolution by CNN\n",
      "16.736856693489266\n",
      ".//LR_img_output//13.jpg high resolution by CNN\n",
      "15.703997716194936\n",
      ".//LR_img_output//130.jpg high resolution by CNN\n",
      "15.750314181823072\n",
      ".//LR_img_output//131.jpg high resolution by CNN\n",
      "12.18269968969958\n",
      ".//LR_img_output//132.jpg high resolution by CNN\n",
      "20.219165500700072\n",
      ".//LR_img_output//133.jpg high resolution by CNN\n",
      "17.956197918805014\n",
      ".//LR_img_output//134.jpg high resolution by CNN\n",
      "15.924104343664624\n",
      ".//LR_img_output//135.jpg high resolution by CNN\n",
      "14.828447913385979\n",
      ".//LR_img_output//136.jpg high resolution by CNN\n",
      "16.626787460228208\n",
      ".//LR_img_output//137.jpg high resolution by CNN\n",
      "15.612929720637954\n",
      ".//LR_img_output//138.jpg high resolution by CNN\n",
      "17.521873025665535\n",
      ".//LR_img_output//139.jpg high resolution by CNN\n",
      "15.587817622363325\n",
      ".//LR_img_output//14.jpg high resolution by CNN\n",
      "16.977517035874776\n",
      ".//LR_img_output//140.jpg high resolution by CNN\n",
      "14.596267006541483\n",
      ".//LR_img_output//141.jpg high resolution by CNN\n",
      "15.599017398412284\n",
      ".//LR_img_output//142.jpg high resolution by CNN\n",
      "14.680501352525736\n"
     ]
    }
   ],
   "source": [
    "original_dir='.//HR_img_4//'\n",
    "original_filenames = [join(original_dir, x) for x in listdir(original_dir) if is_image_file(x)]\n",
    "BICUIBIC_dir='.//LR_img_BICUIBIC//'\n",
    "BICUIBIC_filenames = [join(BICUIBIC_dir, x) for x in listdir(BICUIBIC_dir) if is_image_file(x)]\n",
    "output_dir='.//LR_img_output//'\n",
    "output_filenames = [join(output_dir, x) for x in listdir(output_dir) if is_image_file(x)]\n",
    "face_dir='.//LR_face_img_output//'\n",
    "face_output_filenames = [join(face_dir, x) for x in listdir(face_dir) if is_image_file(x)]\n",
    "face_original_dir='.//face_img_4//'\n",
    "face_filenames = [join(face_original_dir, x) for x in listdir(face_original_dir) if is_image_file(x)]\n",
    "for i in range (0,50):\n",
    "    print(output_filenames[i],'high resolution by CNN')\n",
    "    PSNR(face_filenames[i],face_output_filenames[i])\n",
    "    #print(BICUIBIC_filenames[i],'high resolution by BICUIBIC')\n",
    "    #PSNR(original_filenames[i],BICUIBIC_filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.404926239417456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.404926239417456"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PSNR('.\\\\LR_img_output\\\\14.jpg','.\\\\HR_img_4\\\\14.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output image saved to: .//test_output//14_1.jpg    running time: 1.422717571258545 s\n",
      "18.1453664639979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.1453664639979"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path='.//LR_img_0.25//14.jpg'\n",
    "output_path='.//test_output//14_1.jpg'\n",
    "original_path='.\\\\HR_img_4\\\\14.jpg'\n",
    "output_image(input_dir=input_path,input_model='model_path2.pth',output_dir=output_path)\n",
    "PSNR('.//test_output//14_1.jpg','.\\\\HR_img_4\\\\14.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=load_img(output_path)\n",
    "\n",
    "img2=load_img(original_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_toarray(image):\n",
    "    row,col =  image.size\n",
    "    data = np.zeros((row,col),float)\n",
    "    pixels = image.load()\n",
    "    for i in range(0,row):\n",
    "        for j in range(0,col):\n",
    "            data[i,j] =  pixels[i,j]\n",
    "    return data\n",
    "im1=change_toarray(img1)\n",
    "im2=change_toarray(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.353461568807706\n",
      "18.1453664639979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.1453664639979"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=psnr(im1,im2)\n",
    "print(a)\n",
    "PSNR(output_path,original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1-im2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
