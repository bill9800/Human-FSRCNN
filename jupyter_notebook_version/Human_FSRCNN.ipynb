{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-bcbf885ac1be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeasure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompare_psnr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpsnr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tarfile\n",
    "\n",
    "from os import listdir,remove\n",
    "from os.path import exists, join, basename\n",
    "\n",
    "from six.moves import urllib\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "from math import log10\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "#import argparse\n",
    "import easydict      #rd\n",
    "import cv2\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Prepare Data                                 <br>\n",
    "Download data if can't find it in local directory           <br>\n",
    "Get training set from  './dataset/HR_img_train', get testset from  './dataset/HR_img_test'     <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bsd300(dest=\".//dataset\"):\n",
    "    output_image_dir = join(dest, \"BSDS300/images\")\n",
    "\n",
    "    if not exists(output_image_dir):\n",
    "        url = \"http://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\"\n",
    "        print(\"downloading url \", url)\n",
    "\n",
    "        data = urllib.request.urlopen(url)\n",
    "\n",
    "        file_path = join(dest, basename(url))\n",
    "        with open(file_path, 'wb') as file:\n",
    "            print(\"Extracting data\")\n",
    "        with tarfile.open(file_path) as tar:\n",
    "            for item in tar:\n",
    "                tar.extract(item, dest)\n",
    "\n",
    "        remove(file_path)\n",
    "\n",
    "    return output_image_dir    #rd\n",
    "\n",
    "\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)    #rd    what does this mean?\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])  #rd\n",
    "\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('YCbCr')\n",
    "    y, _, _ = img.split()\n",
    "    return y\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, input_transform=None, target_transform=None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(image_dir, x) for x in listdir(image_dir) if is_image_file(x)]\n",
    "\n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_image = load_img(self.image_filenames[index])\n",
    "        target = input_image.copy()\n",
    "        # print(target.size)\n",
    "        if self.input_transform:\n",
    "            input_image = self.input_transform(input_image)\n",
    "        else:\n",
    "            transi = Compose([\n",
    "                            Resize((input_image.size[1] // 4, input_image.size[0] // 4), Image.BICUBIC),\n",
    "                            ToTensor()\n",
    "                            ])\n",
    "            input_image = transi(input_image)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        else:\n",
    "            transt = Compose([\n",
    "                ToTensor()\n",
    "            ])\n",
    "            target = transt(target)\n",
    "\n",
    "        # print(input_image.size(), target.size())\n",
    "        return input_image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "def input_transform(crop_size, upscale_factor):   #get the low resolution input by original image.  This is a function, return can be viewed\n",
    "                                                  # as a set of parameters import into DatasetFromFolder.input_trainsform and target_transform\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),                        #get a small size of data, Corp with less pieces and resize corp? what does corp mean \n",
    "                                                      # in this project? why not whole img\n",
    "        Resize(crop_size // upscale_factor),           # Resize is what? any help? a way to down size? is it reliable?\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def target_transform(crop_size):\n",
    "    return Compose([\n",
    "        CenterCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(upscale_factor):\n",
    "    # root_dir = download_bsd300()\n",
    "    root_dir = './/dataset//'\n",
    "    train_dir = join(root_dir, \"HR_img_train//\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=None,\n",
    "                             target_transform=None\n",
    "                             # input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             # target_transform=target_transform(crop_size))\n",
    "                             )\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor):\n",
    "    # root_dir = download_bsd300()\n",
    "    root_dir = './/dataset//'\n",
    "    test_dir = join(root_dir, \"HR_img_test//\")\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=None,\n",
    "                             target_transform=None\n",
    "                             # input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             # target_transform=target_transform(crop_size))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Training precedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_BAR_LENGTH = 80\n",
    "LAST_T = time.time()\n",
    "BEGIN_T = LAST_T\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global LAST_T, BEGIN_T\n",
    "    if current == 0:\n",
    "        BEGIN_T = time.time()  # Reset for new bar.\n",
    "\n",
    "    current_len = int(TOTAL_BAR_LENGTH * (current + 1) / total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - current_len) - 1\n",
    "\n",
    "    sys.stdout.write(' %d/%d' % (current + 1, total))\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(current_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    current_time = time.time()\n",
    "    step_time = current_time - LAST_T\n",
    "    LAST_T = current_time\n",
    "    total_time = current_time - BEGIN_T\n",
    "\n",
    "    time_used = '  Step: %s' % format_time(step_time)\n",
    "    time_used += ' | Tot: %s' % format_time(total_time)\n",
    "    if msg:\n",
    "        time_used += ' | ' + msg\n",
    "\n",
    "    msg = time_used\n",
    "    sys.stdout.write(msg)\n",
    "\n",
    "    if current < total - 1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "# return the formatted time\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    seconds_final = int(seconds)\n",
    "    seconds = seconds - seconds_final\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    output = ''\n",
    "    time_index = 1\n",
    "    if days > 0:\n",
    "        output += str(days) + 'D'\n",
    "        time_index += 1\n",
    "    if hours > 0 and time_index <= 2:\n",
    "        output += str(hours) + 'h'\n",
    "        time_index += 1\n",
    "    if minutes > 0 and time_index <= 2:\n",
    "        output += str(minutes) + 'm'\n",
    "        time_index += 1\n",
    "    if seconds_final > 0 and time_index <= 2:\n",
    "        output += str(seconds_final) + 's'\n",
    "        time_index += 1\n",
    "    if millis > 0 and time_index <= 2:\n",
    "        output += str(millis) + 'ms'\n",
    "        time_index += 1\n",
    "    if output == '':\n",
    "        output = '0ms'\n",
    "    return output                      #rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Built CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_channels, upscale_factor, d=64, s=12, m=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.first_part = nn.Sequential(nn.Conv2d(in_channels=num_channels, out_channels=d, kernel_size=5, stride=1, padding=2),\n",
    "                                        nn.PReLU())\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=d, out_channels=s, kernel_size=1, stride=1, padding=0), nn.PReLU()))\n",
    "        for _ in range(m):\n",
    "\n",
    "            # self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1))      # Normal conv\n",
    "\n",
    "            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=3, stride=1, padding=1, groups=s))      # MobileNet conv\n",
    "            self.layers.append(nn.Conv2d(in_channels=s, out_channels=s, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "        self.layers.append(nn.PReLU())\n",
    "        self.layers.append(nn.Sequential(nn.Conv2d(in_channels=s, out_channels=d, kernel_size=1, stride=1, padding=0), nn.PReLU()))\n",
    "\n",
    "        self.mid_part = torch.nn.Sequential(*self.layers)\n",
    "\n",
    "        self.last_part = nn.ConvTranspose2d(in_channels=d, out_channels=num_channels, kernel_size=9, stride=upscale_factor, padding=3, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.first_part(x)\n",
    "        out = self.mid_part(out)\n",
    "        out = self.last_part(out)\n",
    "        return out\n",
    "\n",
    "    def weight_init(self, mean=0.0, std=0.02):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(mean, std)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0.0, 0.0001)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "class FSRCNNTrainer(object):\n",
    "    def __init__(self, config, training_loader, testing_loader):\n",
    "        super(FSRCNNTrainer, self).__init__()\n",
    "        self.CUDA = torch.cuda.is_available()\n",
    "        self.device = torch.device('cuda' if self.CUDA else 'cpu')\n",
    "        self.model = None\n",
    "        self.lr = config.lr\n",
    "        self.nEpochs = config.nEpochs\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.seed = config.seed\n",
    "        self.upscale_factor = config.upscale_factor\n",
    "        self.training_loader = training_loader\n",
    "        self.testing_loader = testing_loader\n",
    "        self.testpsnr = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = Net(num_channels=1, upscale_factor=self.upscale_factor).to(self.device)\n",
    "        self.model.weight_init(mean=0.0, std=0.2)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        if self.CUDA:\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "            cudnn.benchmark = True\n",
    "            self.criterion.cuda()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[50, 75, 100], gamma=0.5)  # lr decay\n",
    "\n",
    "    def save_model(self):\n",
    "        model_out_path = \"model_path.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for batch_num, (data, target) in enumerate(self.training_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(self.model(data), target)\n",
    "            train_loss += loss.item()\n",
    "            # print(train_loss / (batch_num + 1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            #print('batch_num:', batch_num, '/', len(self.training_loader), 'Loss:', train_loss / (batch_num + 1))\n",
    "            progress_bar(batch_num, len(self.training_loader), 'Loss: %.4f' % (train_loss / (batch_num + 1)))\n",
    "\n",
    "        print(\"    Average Loss: {:.4f}\".format(train_loss / len(self.training_loader)))\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        avg_psnr = 0\n",
    "        avg_ssim= 0\n",
    "        avg_psnr2= 0\n",
    "        avg_psnr3= 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (data, target) in enumerate(self.testing_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                prediction = self.model(data)\n",
    "                # psnr\n",
    "                mse = self.criterion(prediction, target)\n",
    "                psnr = 10 * log10(1 / mse.item())\n",
    "                avg_psnr += psnr\n",
    "                # ssim\n",
    "            #    avg_ssim += ssim(target.numpy(),prediction.numpy())\n",
    "                \n",
    "                \n",
    "            progress_bar(batch_num, len(self.testing_loader), 'PSNR: %.4f' % (avg_psnr / (batch_num + 1)))\n",
    "\n",
    "            save_path = './/saved_models//'\n",
    "            if avg_psnr / len(self.testing_loader) > self.testpsnr:\n",
    "                self.testpsnr = avg_psnr / len(self.testing_loader)\n",
    "                folder = os.path.exists(save_path)\n",
    "                if not folder:\n",
    "                    os.makedirs(save_path)\n",
    "                    print('create folder to save models')\n",
    "                torch.save(self.model, save_path + '/model_' + str(self.testpsnr) + '.pkl')\n",
    "                print('model saved ......')\n",
    "            print(\"    Average PSNR: {:.4f} dB\".format(avg_psnr / len(self.testing_loader)))\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.build_model()\n",
    "        print('Total parameters:', sum(p.numel() for p in self.model.parameters()))\n",
    "        print('Total trainable parameters:', sum(p.numel() for p in self.model.parameters() if p.requires_grad))\n",
    "        for epoch in range(1, self.nEpochs + 1):\n",
    "            print(\"\\n===> Epoch {} starts:\".format(epoch))\n",
    "#            self.train()\n",
    "            self.test()\n",
    "            self.scheduler.step(epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model in 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading datasets\n",
      "Total parameters: 9569\n",
      "Total trainable parameters: 9569\n",
      "\n",
      "===> Epoch 1 starts:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-59e0a158714e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-59e0a158714e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFSRCNNTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-c3d8dc3dbb47>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n===> Epoch {} starts:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;31m#            self.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-c3d8dc3dbb47>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                 \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m                 \u001b[1;31m# psnr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-c3d8dc3dbb47>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmid_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_part\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    755\u001b[0m         return F.conv_transpose2d(\n\u001b[0;32m    756\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "#parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "# hyper-parameters\n",
    "#parser.add_argument('--batchSize', type=int, default=4, help='training batch size')\n",
    "#parser.add_argument('--testBatchSize', type=int, default=1, help='testing batch size')\n",
    "#parser.add_argument('--nEpochs', type=int, default=20, help='number of epochs to train for')\n",
    "#parser.add_argument('--lr', type=float, default=0.01, help='Learning Rate. Default=0.01')\n",
    "#parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "# model configuration\n",
    "#parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'batchSize':1,\n",
    "    'testBatchSize':1,\n",
    "   'nEpochs':15,\n",
    "    'lr':0.001,\n",
    "    'seed':123,\n",
    "    'upscale_factor':4,\n",
    "    'uf':4\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ===========================================================\n",
    "    # Set train dataset & test dataset\n",
    "    # ===========================================================\n",
    "    print('===> Loading datasets')\n",
    "    train_set = get_training_set(args.upscale_factor)\n",
    "    test_set = get_test_set(args.upscale_factor)\n",
    "    training_data_loader = DataLoader(dataset=train_set, batch_size=args.batchSize, shuffle=True)\n",
    "    testing_data_loader = DataLoader(dataset=test_set, batch_size=args.testBatchSize, shuffle=False)\n",
    "\n",
    "    model = FSRCNNTrainer(args, training_data_loader, testing_data_loader)\n",
    "\n",
    "    model.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output high resolution image and compute PSNR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output high resolution image to output_dir, with a specified model\n",
    "def output_image(input_dir,input_model,output_dir):\n",
    "#     test = easydict.EasyDict({\n",
    "#         'input':'.//test_image//1.jpg',\n",
    "#         'model':'model_path.pth',              # the loaded model, model should be saved at project folder './mdoel.pth'\n",
    "#        'output':'.//test_image//1_generate_2.jpg',    \n",
    "#     })\n",
    "\n",
    "# ===========================================================\n",
    "# input image setting\n",
    "# ===========================================================\n",
    "    start = time.time()\n",
    "\n",
    "    GPU_IN_USE = torch.cuda.is_available()\n",
    "    img = Image.open(input_dir).convert('YCbCr')\n",
    "    y, cb, cr = img.split()\n",
    "\n",
    "# ===========================================================\n",
    "# model import & setting\n",
    "# ===========================================================\n",
    "    device = torch.device('cuda' if GPU_IN_USE else 'cpu')\n",
    "    model = torch.load(input_model, map_location=lambda storage, loc: storage)\n",
    "    model = model.to(device)\n",
    "    data = (ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n",
    "    data = data.to(device)\n",
    "\n",
    "    if GPU_IN_USE:\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# output and save image\n",
    "# ===========================================================\n",
    "    out = model(data)\n",
    "    out = out.cpu()\n",
    "    out_img_y = out.data[0].numpy()\n",
    "    out_img_y *= 255.0\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
    "\n",
    "    out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
    "\n",
    "    out_img.save(output_dir)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    print('output image saved to:', output_dir,'   running time:',elapsed,'s')\n",
    "\n",
    "# output high resolution image to output_dir, with the current model\n",
    "def creat_image(input_dir,output_dir):\n",
    "    GPU_IN_USE = torch.cuda.is_available()\n",
    "   \n",
    "    start = time.time()\n",
    "    \n",
    "    img = Image.open(input_dir).convert('YCbCr')\n",
    "    y, cb, cr = img.split()\n",
    "    \n",
    "\n",
    " #   model = torch.load(input_model, map_location=lambda storage, loc: storage)\n",
    " #   model = model.to(device)\n",
    "    data = (ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n",
    "    data = data.to(device)\n",
    "\n",
    "    if GPU_IN_USE:\n",
    "        cudnn.benchmark = True\n",
    "        print('lalallalala')\n",
    "    out = model(data)\n",
    "    out = out.cpu()\n",
    "    out_img_y = out.data[0].numpy()\n",
    "    out_img_y *= 255.0\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
    "\n",
    "    out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
    "\n",
    "    out_img.save(output_dir)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    #print('output image saved to:', output_dir,'   running time:',elapsed,'s')\n",
    "    \n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-7ffd337ef136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mGPU_IN_USE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mGPU_IN_USE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "#test_model_path='.//saved_models//aug_full_lr_0.001//model_38.pth'\n",
    "#test_model_path='.//saved_models//model_path2.pth'\n",
    "test_model_path='.//saved_models//mobilnet_batch32//model_36.pth'\n",
    "GPU_IN_USE = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if GPU_IN_USE else 'cpu')\n",
    "model = torch.load(test_model_path, map_location='cpu')\n",
    "model = model1.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test speed and performance of different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_img(source_path,target_path):\n",
    "    src_img = cv2.imread(source_path)\n",
    "    tar_img = cv2.imread(target_path)\n",
    "    ssim_const = ssim(src_img,tar_img,multichannel=True)\n",
    "    psnr_const = psnr(src_img,tar_img)\n",
    "    print('ssim : ',ssim_const)\n",
    "    print('psnr : ',psnr_const)\n",
    "    return ssim_const,psnr_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-3fbcc8383c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimage_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_test_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_test_dir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_model_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mavg_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-67d341f10a6a>\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mGPU_IN_USE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mGPU_IN_USE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch_env\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[0;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "test_model_path='.//saved_models//mobilnet_batch32//model_36.pth'\n",
    "\n",
    "img_test_dir='.//test_low//'\n",
    "img_original_dir='.//test_orginal//'\n",
    "img_output_dir='.//test_output_image//'\n",
    "\n",
    "image_filenames = [join(img_test_dir, x) for x in listdir(img_test_dir) if is_image_file(x)]\n",
    "\n",
    "model=load_model(test_model_path)\n",
    " \n",
    "avg_time=0\n",
    "avg_psnr=0\n",
    "avg_ssim=0\n",
    "\n",
    "for i,filename in enumerate (image_filenames):\n",
    "\n",
    "    img_name=os.path.basename(filename)\n",
    "    print(img_name)\n",
    "    avg_time+=creat_image(input_dir=filename,output_dir=img_output_dir+img_name)\n",
    "\n",
    "    new_ssim,new_psnr=compare_img(img_original_dir+img_name,img_output_dir+img_name)\n",
    "    avg_ssim+=new_ssim\n",
    "    avg_psnr+=new_psnr\n",
    "#    avg_ssim+=ssim(img_original_dir+img_name,img_output_dir+filename+img_name)\n",
    "\n",
    "print('%f ,%f ,%f'%(avg_time/len(image_filenames),avg_psnr/len(image_filenames),avg_ssim/len(image_filenames)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".//LR_img_output//0.jpg high resolution by CNN\n",
      "16.27083686424689\n",
      ".//LR_img_output//1.jpg high resolution by CNN\n",
      "15.861659877553185\n",
      ".//LR_img_output//10.jpg high resolution by CNN\n",
      "18.275048959429615\n",
      ".//LR_img_output//100.jpg high resolution by CNN\n",
      "16.040215030275483\n",
      ".//LR_img_output//101.jpg high resolution by CNN\n",
      "19.14871264652683\n",
      ".//LR_img_output//102.jpg high resolution by CNN\n",
      "17.68598291436126\n",
      ".//LR_img_output//103.jpg high resolution by CNN\n",
      "18.355294170829158\n",
      ".//LR_img_output//104.jpg high resolution by CNN\n",
      "19.102057471809392\n",
      ".//LR_img_output//105.jpg high resolution by CNN\n",
      "11.948134469080374\n",
      ".//LR_img_output//106.jpg high resolution by CNN\n",
      "17.453424681585588\n",
      ".//LR_img_output//107.jpg high resolution by CNN\n",
      "17.81386131065656\n",
      ".//LR_img_output//108.jpg high resolution by CNN\n",
      "18.52581514150305\n",
      ".//LR_img_output//109.jpg high resolution by CNN\n",
      "13.165928528115018\n",
      ".//LR_img_output//11.jpg high resolution by CNN\n",
      "19.383291008033115\n",
      ".//LR_img_output//110.jpg high resolution by CNN\n",
      "19.35636073497009\n",
      ".//LR_img_output//111.jpg high resolution by CNN\n",
      "18.22341090248054\n",
      ".//LR_img_output//112.jpg high resolution by CNN\n",
      "17.012997224514116\n",
      ".//LR_img_output//113.jpg high resolution by CNN\n",
      "17.683215750720947\n",
      ".//LR_img_output//114.jpg high resolution by CNN\n",
      "20.98048083838585\n",
      ".//LR_img_output//115.jpg high resolution by CNN\n",
      "12.409886953890215\n",
      ".//LR_img_output//116.jpg high resolution by CNN\n",
      "16.97531456426456\n",
      ".//LR_img_output//117.jpg high resolution by CNN\n",
      "15.61192309053553\n",
      ".//LR_img_output//118.jpg high resolution by CNN\n",
      "15.593098204303457\n",
      ".//LR_img_output//119.jpg high resolution by CNN\n",
      "17.201756112018003\n",
      ".//LR_img_output//12.jpg high resolution by CNN\n",
      "12.255609249671869\n",
      ".//LR_img_output//120.jpg high resolution by CNN\n",
      "14.89019242665454\n",
      ".//LR_img_output//121.jpg high resolution by CNN\n",
      "16.108570774106937\n",
      ".//LR_img_output//122.jpg high resolution by CNN\n",
      "12.005458752324165\n",
      ".//LR_img_output//123.jpg high resolution by CNN\n",
      "13.77058612695253\n",
      ".//LR_img_output//124.jpg high resolution by CNN\n",
      "15.57636267953817\n",
      ".//LR_img_output//125.jpg high resolution by CNN\n",
      "20.52250509605484\n",
      ".//LR_img_output//126.jpg high resolution by CNN\n",
      "15.20427178585441\n",
      ".//LR_img_output//127.jpg high resolution by CNN\n",
      "14.018167439974105\n",
      ".//LR_img_output//128.jpg high resolution by CNN\n",
      "14.242707670040758\n",
      ".//LR_img_output//129.jpg high resolution by CNN\n",
      "16.736856693489266\n",
      ".//LR_img_output//13.jpg high resolution by CNN\n",
      "15.703997716194936\n",
      ".//LR_img_output//130.jpg high resolution by CNN\n",
      "15.750314181823072\n",
      ".//LR_img_output//131.jpg high resolution by CNN\n",
      "12.18269968969958\n",
      ".//LR_img_output//132.jpg high resolution by CNN\n",
      "20.219165500700072\n",
      ".//LR_img_output//133.jpg high resolution by CNN\n",
      "17.956197918805014\n",
      ".//LR_img_output//134.jpg high resolution by CNN\n",
      "15.924104343664624\n",
      ".//LR_img_output//135.jpg high resolution by CNN\n",
      "14.828447913385979\n",
      ".//LR_img_output//136.jpg high resolution by CNN\n",
      "16.626787460228208\n",
      ".//LR_img_output//137.jpg high resolution by CNN\n",
      "15.612929720637954\n",
      ".//LR_img_output//138.jpg high resolution by CNN\n",
      "17.521873025665535\n",
      ".//LR_img_output//139.jpg high resolution by CNN\n",
      "15.587817622363325\n",
      ".//LR_img_output//14.jpg high resolution by CNN\n",
      "16.977517035874776\n",
      ".//LR_img_output//140.jpg high resolution by CNN\n",
      "14.596267006541483\n",
      ".//LR_img_output//141.jpg high resolution by CNN\n",
      "15.599017398412284\n",
      ".//LR_img_output//142.jpg high resolution by CNN\n",
      "14.680501352525736\n"
     ]
    }
   ],
   "source": [
    "original_dir='.//HR_img_4//'\n",
    "original_filenames = [join(original_dir, x) for x in listdir(original_dir) if is_image_file(x)]\n",
    "BICUIBIC_dir='.//LR_img_BICUIBIC//'\n",
    "BICUIBIC_filenames = [join(BICUIBIC_dir, x) for x in listdir(BICUIBIC_dir) if is_image_file(x)]\n",
    "output_dir='.//LR_img_output//'\n",
    "output_filenames = [join(output_dir, x) for x in listdir(output_dir) if is_image_file(x)]\n",
    "face_dir='.//LR_face_img_output//'\n",
    "face_output_filenames = [join(face_dir, x) for x in listdir(face_dir) if is_image_file(x)]\n",
    "face_original_dir='.//face_img_4//'\n",
    "face_filenames = [join(face_original_dir, x) for x in listdir(face_original_dir) if is_image_file(x)]\n",
    "for i in range (0,50):\n",
    "    print(output_filenames[i],'high resolution by CNN')\n",
    "    PSNR(face_filenames[i],face_output_filenames[i])\n",
    "    #print(BICUIBIC_filenames[i],'high resolution by BICUIBIC')\n",
    "    #PSNR(original_filenames[i],BICUIBIC_filenames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output image saved to: .//test_output//14_1.jpg    running time: 1.422717571258545 s\n",
      "18.1453664639979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.1453664639979"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path='.//LR_img_0.25//14.jpg'\n",
    "output_path='.//test_output//14_1.jpg'\n",
    "original_path='.\\\\HR_img_4\\\\14.jpg'\n",
    "output_image(input_dir=input_path,input_model='model_path2.pth',output_dir=output_path)\n",
    "PSNR('.//test_output//14_1.jpg','.\\\\HR_img_4\\\\14.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
